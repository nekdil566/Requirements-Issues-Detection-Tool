{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nekdil566/Requirements-Issues-Detection-Tool/blob/main/GPT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://colab.research.google.com/drive/1BHL_jaEVuTy6cfYBk8n-8iYvSeJICo12?usp=sharing#scrollTo=Yc6pgsbUCnLj"
      ],
      "metadata": {
        "id": "Q2DY-spcs4SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rNR8d4vWKqV"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "25eEw-KzYZRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "23X_k7MhYZTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Record Each Cell's Execution Time\n",
        "!pip install ipython-autotime\n",
        "\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "2ASCW9iYYZVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "-SgwQL7EYZcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\", encoding='latin1', usecols=['Base_Reviews','category'])\n",
        "data = data[pd.notnull(data['category'])]\n",
        "print(data['Base_Reviews'].astype(str).apply(lambda x: len(x.split(' '))).sum())"
      ],
      "metadata": {
        "id": "HxhIZl6_YZei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "79dubMZfxD3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "gq_R3S9JYZge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(5)"
      ],
      "metadata": {
        "id": "C6j8FQAAYZk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.My_Labels.value_counts()"
      ],
      "metadata": {
        "id": "MaSupZHX46ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "RQ_ijdVIYZm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.Base_Reviews.duplicated(keep=\"first\").value_counts()"
      ],
      "metadata": {
        "id": "cNgPkfsJYZpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop_duplicates(subset=\"Base_Reviews\",keep=\"first\",inplace=True,ignore_index=True)\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "UMwcbnDWYZrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.My_Labels.value_counts()"
      ],
      "metadata": {
        "id": "cDMe8ptbYZtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8,6))\n",
        "data.groupby('category').Base_Reviews.count().plot.bar(ylim=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5i44SAROaWS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['words'] = [len(x.split()) for x in data['Base_Reviews'].tolist()]"
      ],
      "metadata": {
        "id": "mXnhJr5vaWaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail(5)"
      ],
      "metadata": {
        "id": "z0BVt5k7aWcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['words'].describe()"
      ],
      "metadata": {
        "id": "OFtv2E5jaWfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of words in each review\n",
        "data.Base_Reviews.str.split().\\\n",
        "    map(lambda x: len(x)).\\\n",
        "    hist()"
      ],
      "metadata": {
        "id": "_zvNFPj-aWhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max(data['Base_Reviews'], key=len))"
      ],
      "metadata": {
        "id": "JrlM3-LEaWjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(min(data['Base_Reviews'], key=len))"
      ],
      "metadata": {
        "id": "GIH8SGfdaWlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Base_Reviews\"]"
      ],
      "metadata": {
        "id": "YzqdGQkhaWne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"My_Labels\"]"
      ],
      "metadata": {
        "id": "RnWe5BWPaWqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"category\"] = data[\"category\"].astype('category') #By converting an existing Series or column to a category dtype\n",
        "data.dtypes"
      ],
      "metadata": {
        "id": "HIBoAhzebh7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"category_id\"] = data[\"category\"].cat.codes\n",
        "data.tail()"
      ],
      "metadata": {
        "id": "IiCo-gT9bh9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dtypes"
      ],
      "metadata": {
        "id": "MZN6sA6ybiCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_category = pd.Series(data.category.values,index=data.category_id).to_dict()\n",
        "id_to_category"
      ],
      "metadata": {
        "id": "WF9mRolCbiEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_to_id= {v:k for k,v in id_to_category.items()}\n",
        "category_to_id"
      ],
      "metadata": {
        "id": "LV5lznldbiGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_categories = len(category_to_id)\n",
        "number_of_categories"
      ],
      "metadata": {
        "id": "HTuM1xQBbiIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordList = list()\n",
        "for i in range(len(data)):\n",
        "    temp = data.Base_Reviews[i].split()\n",
        "    for k in temp:\n",
        "        k = re.sub(\"[^a-zA-ZğĞüÜşŞıİöÖçÇ]\",\"\",k)\n",
        "        if k != \"\":\n",
        "            wordList.append(k)"
      ],
      "metadata": {
        "id": "578E9GBsbiQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordCount = Counter(wordList)\n",
        "countedWordDict = dict(wordCount)\n",
        "sortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\n",
        "\n",
        "print(\"Most Used 20 Words\")\n",
        "for word,counted in sortedWordDict[0:20]:\n",
        "    print(\"{} : {}\".format(word,counted))"
      ],
      "metadata": {
        "id": "RstlnSl6bieQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in data[\"Base_Reviews\"][7:10]:\n",
        "    if \"oku\" in i:\n",
        "        print(i)\n",
        "        print(\"*\"*20)"
      ],
      "metadata": {
        "id": "nvNR64HbbihW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O_l0lBOQbiln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us take a look at the most frequently used word in the reviews\n",
        "from wordcloud import WordCloud\n",
        "word_cloud = WordCloud(width = 1000,\n",
        "                       height = 800,\n",
        "                       colormap = 'Blues', \n",
        "                       margin = 0,\n",
        "                       max_words = 183,  \n",
        "                       max_font_size = 120, min_font_size = 15,  \n",
        "                       background_color = \"white\").generate(\" \".join(data['Base_Reviews']))\n",
        "\n",
        "plt.figure(figsize = (10, 15))\n",
        "plt.imshow(word_cloud, interpolation = \"gaussian\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3H-1py3dbinq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = data.sample(1000, random_state=1).copy()"
      ],
      "metadata": {
        "id": "Gm_ydTvbbip5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pBGC3dFSbisV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
        "                        ngram_range=(1,1))\n",
        "\n",
        "# We transform each review into a vector\n",
        "df2_features = tfidf.fit_transform(df2.Base_Reviews).toarray()\n",
        "\n",
        "df2_labels = df2.category_id\n",
        "\n",
        "print(\"Each of the %d reviews is represented by %d features (TF-IDF score of unigrams and bigrams)\" %(df2_features.shape))"
      ],
      "metadata": {
        "id": "Gexungssbiu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2_features.shape"
      ],
      "metadata": {
        "id": "to2a4wk7biwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding the three most correlated terms with each of the categories\n",
        "N = 20\n",
        "for category, category_id in sorted(category_to_id.items()):\n",
        "  features_chi2 = chi2(df2_features, df2_labels == category_id)\n",
        "  indices = np.argsort(features_chi2[0])\n",
        "  feature_names = np.array(tfidf.get_feature_names())[indices]\n",
        "  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
        "  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
        "  print(\"\\n==> %s:\" %(category))\n",
        "  print(\"  * Most Correlated Unigrams are: %s\" %(', '.join(unigrams[-N:])))\n",
        "  print(\"  * Most Correlated Bigrams are: %s\" %(', '.join(bigrams[-N:])))"
      ],
      "metadata": {
        "id": "_20lCsrdbiy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#limit the number of samples to be used in code runs\n",
        "data_size= 9870"
      ],
      "metadata": {
        "id": "AaqOSNyqbi1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 2000  # Only consider the top 90k words\n",
        "maxlen = 50  # Max sequence size                           # orignale 50 "
      ],
      "metadata": {
        "id": "gqwiU7PPbi3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save features and targets from the 'data' (raw data)\n",
        "features, targets = data['Base_Reviews'], data['category_id']"
      ],
      "metadata": {
        "id": "Hz-bPrEobi5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features, test_features, train_targets, test_targets = train_test_split(\n",
        "        features, targets,\n",
        "        train_size=0.8,\n",
        "        test_size=0.2,\n",
        "        # random but same for all run, also accurancy depends on the\n",
        "        # selection of data e.g. if we put 10 then accuracy will be 1.0\n",
        "        # in this example\n",
        "        random_state=23,\n",
        "        # keep same proportion of 'target' in test and target data\n",
        "        stratify=targets\n",
        "    )"
      ],
      "metadata": {
        "id": "U7bz8JgHbi75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_string): \n",
        "    first_input = tf.strings.regex_replace(input_string, \"İ\", \"I\")\n",
        "    lowercased = tf.strings.lower(first_input, encoding='utf-8') #Turn to lower case\n",
        "    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \") #Remove html line-break tags\n",
        "    no_numbers = tf.strings.regex_replace(stripped_html, \"\\w*\\d\\w*\",\" \") #Remove numbers\n",
        "    no_punctuation = tf.strings.regex_replace(no_numbers,'[%s]' % re.escape(string.punctuation),'') #Remove punctuations\n",
        "    no_turkish_character = tf.strings.regex_replace(no_punctuation, \"ç\", \"c\") #Replace turkish characters\n",
        "    no_turkish_character = tf.strings.regex_replace(no_turkish_character, \"ğ\", \"g\")\n",
        "    no_turkish_character = tf.strings.regex_replace(no_turkish_character, \"ı\", \"i\")\n",
        "    no_turkish_character = tf.strings.regex_replace(no_turkish_character, \"ö\", \"o\")\n",
        "    no_turkish_character = tf.strings.regex_replace(no_turkish_character, \"ş\", \"s\")\n",
        "    no_turkish_character = tf.strings.regex_replace(no_turkish_character, \"ü\", \"u\")\n",
        "    no_read_more = tf.strings.regex_replace(no_turkish_character, \"...devamini oku\", \" \") #Remove \"Read More\"\n",
        "    return no_read_more"
      ],
      "metadata": {
        "id": "lg17VScXbi9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output = custom_standardization('Pijamalı İlginç Otobüs Çıkıp Öldü Gözleri')\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "ErJ6bi8Xbi_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen,\n",
        ")"
      ],
      "metadata": {
        "id": "R4bbRm3XbjB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(train_features)\n",
        "vocab = vectorize_layer.get_vocabulary() "
      ],
      "metadata": {
        "id": "mTXUQaW0bjD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocab has the \", len(vocab),\" entries\")\n",
        "print(\"vocab has the following first 10 entries\")\n",
        "for word in range(10):\n",
        "  print(word, \" represents the word: \", vocab[word])"
      ],
      "metadata": {
        "id": "rK3Ax4jzaWsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector for word \"good\"\n",
        "print(vectorize_layer(\"good\"))"
      ],
      "metadata": {
        "id": "O6F3pWRDaWt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector for word \"bu güzel gün\"\n",
        "print(vectorize_layer(\"Nothing dog about it. It's like a spa video\"))"
      ],
      "metadata": {
        "id": "P29VA1h6aWwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pickle the config and weights\n",
        "pickle.dump({'config': vectorize_layer.get_config(),\n",
        "             'weights': vectorize_layer.get_weights()}\n",
        "            , open(\"tv_layer.pkl\", \"wb\"))\n",
        "\n",
        "# Later you can unpickle and use \n",
        "# `config` to create object and \n",
        "# `weights` to load the trained weights. "
      ],
      "metadata": {
        "id": "O_dJwcfsYZwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from_disk = pickle.load(open(\"tv_layer.pkl\", \"rb\"))\n",
        "new_vectorize_layer = TextVectorization.from_config(from_disk['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "new_vectorize_layer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "new_vectorize_layer.set_weights(from_disk['weights'])"
      ],
      "metadata": {
        "id": "WlzY1IuUBw8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector for word \"bu\"\n",
        "print(new_vectorize_layer(\"good\"))"
      ],
      "metadata": {
        "id": "k7DIHAorBw-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector for word \"bu güzel gün\"\n",
        "print(new_vectorize_layer(\"Nothing dog about it. It's like a spa video\"))"
      ],
      "metadata": {
        "id": "HZOp_mgXBxBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector for word \"bu güzel gün\"\n",
        "print(new_vectorize_layer(\"Nothing dog about it. It's like a spa video\"))\n"
      ],
      "metadata": {
        "id": "8zhb9HZVBxDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim=embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim =ff_dim\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        # for masked-self attention add the mask:\n",
        "        # causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        # attention_output = self.att(inputs, inputs,attention_mask=causal_mask)\n",
        "        \n",
        "        attention_output = self.att(inputs, inputs) \n",
        "        \n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "    \n",
        "    # https://newbedev.com/saving-keras-models-with-custom-layers\n",
        "    def get_config(self):\n",
        "        config = super(TransformerBlock, self).get_config()\n",
        "        config.update({\n",
        "            'att': self.att,\n",
        "            'ffn': self.ffn,\n",
        "            'layernorm1': self.layernorm1,\n",
        "            'layernorm2':self.layernorm2,\n",
        "            'dropout1':self.dropout1,\n",
        "            'dropout2':self.dropout2,\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads':self.num_heads,\n",
        "            'ff_dim':self.ff_dim \n",
        "\n",
        "        })\n",
        "        return config\n",
        "        #tf.keras.models.save_model(model, 'model.h5')\n",
        "        #new_model = tf.keras.models.load_model('model.h5', custom_objects={'CustomLayer': CustomLayer})"
      ],
      "metadata": {
        "id": "8LVJ-qO1BxFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size,  embed_dim, **kwargs):\n",
        "        super(TokenPositionEmbedding, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, X):\n",
        "        maxlen = tf.shape(X)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        X = self.token_emb(X)\n",
        "        return X + positions \n",
        "        \n",
        "    # https://newbedev.com/saving-keras-models-with-custom-layers\n",
        "    def get_config(self):\n",
        "        config = super(TokenPositionEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            'token_emb': self.token_emb,\n",
        "            'pos_emb': self.pos_emb,\n",
        "            'maxlen': self.maxlen,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim\n",
        "        })\n",
        "        return config\n",
        "        #tf.keras.models.save_model(model, 'model.h5')\n",
        "        #new_model = tf.keras.models.load_model('model.h5', custom_objects={'CustomLayer': CustomLayer})\n",
        "        "
      ],
      "metadata": {
        "id": "YEPfL90DBxHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    inputs_tokens = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = TokenPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs_tokens)\n",
        "    transformer_block1 = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    transformer_block2 = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
        "    x = transformer_block1(x)\n",
        "    x = transformer_block2(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    outputs = layers.Dense(number_of_categories)(x)\n",
        "    model = keras.Model(inputs=inputs_tokens, outputs=outputs)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric_fn  = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=metric_fn)  \n",
        "    \n",
        "    return model\n",
        "my_model=create_model()"
      ],
      "metadata": {
        "id": "_ovmR358BxJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.summary()"
      ],
      "metadata": {
        "id": "2C3H9zlnBxMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(my_model,show_shapes=True)"
      ],
      "metadata": {
        "id": "4z-PDTvYBxNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = './checkpoint'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False)"
      ],
      "metadata": {
        "id": "u1zyw54VBxQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = train_features,train_targets\n",
        "\n",
        "balanced_accuracy_scores = []\n",
        "matthews_corrcoef_scores = []\n",
        "f1_scores = []\n",
        "conf_matrix_list_of_arrays = []\n",
        "\n",
        "# prepare cross validation\n",
        "n=10\n",
        "seed=1\n",
        "k_fold = KFold(n_splits=n, random_state=seed, shuffle=True)\n",
        "\n",
        "for train_index, test_index in k_fold.split(X,y):\n",
        "  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "  X_train_dtm = vectorize_layer(X_train)\n",
        "  X_test_dtm = vectorize_layer(X_test)\n",
        "  history = my_model.fit(X_train_dtm, y_train, verbose=1, epochs=1, callbacks=[model_checkpoint_callback])\n",
        "  y_pred_class = my_model.predict(X_test_dtm)\n",
        "\n",
        "  y_pred_class = np.argmax(y_pred_class,axis=1)\n",
        "  \n",
        "  conf_matrix = confusion_matrix(y_test, y_pred_class)\n",
        "  conf_matrix_list_of_arrays.append(conf_matrix)\n",
        "  \n",
        "  balanced_accuracy_scores.append(balanced_accuracy_score(y_test, y_pred_class))\n",
        "  matthews_corrcoef_scores.append(matthews_corrcoef(y_test, y_pred_class))\n",
        "  f1_scores.append(f1_score(y_test, y_pred_class, average='weighted'))"
      ],
      "metadata": {
        "id": "-u0hsGj3BxSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_of_conf_matrix_arrays = np.mean(conf_matrix_list_of_arrays, axis=0)\n",
        "print('Mean of conf_matrix: ', mean_of_conf_matrix_arrays)"
      ],
      "metadata": {
        "id": "9Pqm8GupBxUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_mat = mean_of_conf_matrix_arrays.astype(int)"
      ],
      "metadata": {
        "id": "sDbOqyYDHL_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_id_df = data[['category', 'category_id']].drop_duplicates()"
      ],
      "metadata": {
        "id": "pjPfESYWHMCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "sns.heatmap(conf_mat, annot=True, cmap=\"Blues\", fmt='d',\n",
        "            xticklabels=category_id_df.category.values, \n",
        "            yticklabels=category_id_df.category.values)\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.title(\"CONFUSION MATRIX \\n\", size=16);"
      ],
      "metadata": {
        "id": "RW25H5ruHMEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_accuracy_scores = np.array(balanced_accuracy_scores)\n",
        "print('Mean of balanced_accuracy_scores: ', np.mean(balanced_accuracy_scores, axis=0))"
      ],
      "metadata": {
        "id": "PoGDMedaHMGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matthews_corrcoef_scores = np.array(matthews_corrcoef_scores)\n",
        "print('Mean of matthews_corrcoef_scores: ', np.mean(matthews_corrcoef_scores, axis=0))"
      ],
      "metadata": {
        "id": "6P5X8LZFHr6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_scores = np.array(f1_scores)\n",
        "print('Mean of f1_scores: ', np.mean(f1_scores, axis=0))"
      ],
      "metadata": {
        "id": "-alEzPq0Hr9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features_vc = vectorize_layer(test_features)\n",
        "y_pred_test = my_model.predict(test_features_vc)\n",
        "y_pred_test = np.argmax(y_pred_test,axis=1)\n",
        " \n",
        "balanced_accuracy_score_test = balanced_accuracy_score(test_targets, y_pred_test)\n",
        "print('balanced_accuracy_score_test :',balanced_accuracy_score_test)\n",
        "matthews_corrcoef_score_test = matthews_corrcoef(test_targets, y_pred_test)\n",
        "print('matthews_corrcoef_score_test :',matthews_corrcoef_score_test)\n",
        "f1_score_test = f1_score(test_targets, y_pred_test, average='weighted')\n",
        "print('f1_score_test :',f1_score_test)\n"
      ],
      "metadata": {
        "id": "j9eptFPzHr_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_review = \"I removed it.\"\n",
        "predictions=my_model.predict(vectorize_layer([new_review]))\n",
        "for pred in predictions:\n",
        "  print(id_to_category[np.argmax(pred)])"
      ],
      "metadata": {
        "id": "yxWgX0zvHsBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "  \"I removed it.\",\n",
        "  \"Does what it says great app!\",\n",
        "  \"Not compatable.\",\n",
        "  \"Ok.\",\n",
        "  \"don't waste ur time , this app won't help ur Amazon Tablet run smoother or faster.  skip it\",\n",
        "]\n",
        "predictions=my_model.predict(vectorize_layer(examples))\n",
        "for pred in predictions:\n",
        "  print(id_to_category[np.argmax(pred)])"
      ],
      "metadata": {
        "id": "3lmNbyBSHsEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.models.save_model(my_model, 'MultiClassTextClassifier')"
      ],
      "metadata": {
        "id": "jh6UB9SYHsGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}